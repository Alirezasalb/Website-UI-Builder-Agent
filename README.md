#  Website(UI) Builder Agent

An AI-powered autonomous agent that designs and builds responsive websites from natural language prompts â€” using a **LangGraph workflow**, **multi-agent collaboration**, and **real-time code generation**.

> âœ¨ *Describe a UI â†’ Agent plans, writes, and deploys live HTML/CSS/JS â€” all in your browser.*



---

## ðŸŒŸ Features

- **Natural Language to UI**: â€œCreate a dark-themed landing page with a hero section and CTA buttonâ€ â†’ full website.
- **Autonomous Workflow**: Router â†’ Planner â†’ Code Agent â†’ Validation loop.
- **Live Sandbox Preview**: Real-time iframe rendering of generated code.
- **Stateful Session**: Chat history, iterative refinement, and code updates.
- **Local & Private**: Runs entirely on your machine â€” no external APIs required.
- **Optimized for High VRAM GPUs**: Designed for NVIDIA RTX A6000 (48GB VRAM) with vLLM backend.

---

## ðŸ—ï¸ Architecture

```mermaid
graph LR
A[User Prompt] --> B(Router Agent)
B -->|Plan| C(Planner Agent)
C --> D(Code Agent)
D -->|Save Code| E[website_sandbox/]
E --> F[Sandbox Preview]
D --> B{Check Completion}
B -->|End| G[Final Output]
```

- **LangGraph State Machine**: Orchestrates agent coordination and loop logic.
- **vLLM Backend**: Serves `Qwen/Qwen3-14B` with high throughput & low latency.
- **FastAPI + Jinja2**: Web UI with clean dark-blue theme and real-time updates.
- **File-Based Sandbox**: Generated website served directly from `app/website_sandbox/`.

---

## ðŸ› ï¸ Requirements

### Hardware (Recommended)
- **GPU**: NVIDIA RTX A6000 (48GB VRAM)
- **System RAM**: â‰¥ 32 GB
- **Storage**: â‰¥ 30 GB free (model + dependencies)

### Software
- Python â‰¥ 3.10
- CUDA â‰¥ 12.1
- vLLM â‰¥ 0.4.3
- PyTorch â‰¥ 2.3
- Hugging Face `transformers`, `tokenizers`
- `langchain`, `langgraph`, `fastapi`, `uvicorn`, `jinja2`

---

##  Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/Alirezasalb/Website-UI-Builder-Agent.git
cd Website-UI-Builder-Agent

# Virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

pip install -r requirements.txt
```



---

### 2. Launch vLLM Server (Qwen3-14B)

> âš ï¸ **Run this FIRST in a dedicated terminal** â€” it takes 2â€“5 minutes to load on A6000.

```bash
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-14B \
  --host 0.0.0.0 \
  --port 7052 \
  --dtype bfloat16 \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.9 \
  --trust-remote-code
```

âœ… **A6000 Optimization**:
- `--dtype bfloat16`: Best speed/accuracy trade-off.
- `--gpu-memory-utilization 0.9`: Uses ~43GB VRAM â€” safe for 48GB card.
- `--max-model-len 32768`: Supports long UI descriptions and code.


---

### 3. Launch Web UI (FastAPI)

In a **second terminal**, activate your venv and run:

```bash
uvicorn app.web.main:app \
  --host 0.0.0.0 \
  --port 7051 \
  --timeout-keep-alive 300
```

Then open:  
ðŸ‘‰ [http://localhost:7051](http://localhost:7051)

---

## ðŸ§ª Usage

1. Type a prompt:  
   > *â€œBuild a responsive pricing page with three tiers, gradient buttons, and dark mode.â€*

2. Click **ðŸš€ Send** â€” the agent will:
   - Route the request
   - Generate HTML/CSS/JS
   - Save files to `app/website_sandbox/`
   - Update chat history & code view
   - Auto-refresh the sandbox preview

3. Iterate:  
   > *â€œMake the middle plan highlighted with a pulsing border.â€*

âœ… The `code-editor` textarea and live iframe update automatically.

---

## ðŸ“ Project Structure

```
app/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes.py                 # HTTP endpoints: /, /process_request, /sandbox/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agents.py                 # Router, Planner, Code Agent nodes
â”‚   â”œâ”€â”€ graph.py                  # LangGraph workflow definition
â”‚   â”œâ”€â”€ llm.py                    # LLM client (points to vLLM OpenAI API)
â”‚   â””â”€â”€ state.py                  # AgentState TypedDict
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ code_editor.py            # Reads/writes files in website_sandbox/
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ main.py                   # FastAPI app entrypoint
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ css/styles.css        # Dark, modern UI theme
â”‚   â”‚   â””â”€â”€ js/script.js          # Cache-busting sandbox refresh
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html            # Jinja2 frontend template
â””â”€â”€ website_sandbox/              # ðŸŒ Generated website (index.html, style.css, script.js)
```

---

## ðŸ“ Notes

- **LLM Endpoint**: `app/core/llm.py` must point to `http://localhost:7052/v1` (vLLM OpenAI API).
- **Sandbox Serving**: `/sandbox/*` routes serve files from `website_sandbox/` with correct MIME types.
- **State Management**: Uses in-memory global state (for demo); replace with Redis/session for production.
- **Security**: Do **not** expose ports `7051`/`7052` publicly â€” this is a local dev tool.
- **NDA Compliance**: Internal logic and company-specific enhancements are protected.

---

## ðŸ“œ License

This project is for demonstration, research, and internal use only.  
Â© 2025 Alireza Salbizadeh. â€” All rights reserved.

---

## ðŸ™Œ Acknowledgements

- [vLLM](https://vllm.readthedocs.io) â€” for efficient, scalable LLM serving  
- [LangGraph](https://langchain-ai.github.io/langgraph/) â€” for stateful, cyclic agent workflows  
- [Qwen](https://qwenlm.github.io) â€” outstanding open-source LLM by Alibaba  
- Inspired by autonomous UI agents, RAG, and tool-use patterns in modern LLM systems.

---

> ðŸ”® *The future of frontend development is conversational.*  
> Try it. Iterate. Ship.
```